{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "elder-biotechnology",
   "metadata": {},
   "source": [
    "# U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nibabel as nib \n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "\n",
    "data_dir = '/data/projects/ccm/MRIs'\n",
    "id_1     = 1\n",
    "id_2     = 15\n",
    "\n",
    "image = nib.load(f'{data_dir}/train/img_subj{id_1:>03}_sl_{id_2:>03}.nii').get_data()\n",
    "msk = nib.load(f'{data_dir}/train/roi_subj{id_1:>03}_sl_{id_2:>03}.nii').get_data()\n",
    "msk[msk==2] = 0\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 10))\n",
    "ax[0].imshow(image, cmap='gray')\n",
    "ax[1].imshow(msk, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ee674",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bcc1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(conv_block,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(up_conv,self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818f4a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class single_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(single_conv,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4579814",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self,img_ch=1,output_ch=1):\n",
    "        super(Unet,self).__init__()\n",
    "        \n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(ch_in=img_ch,ch_out=64)\n",
    "        self.Conv2 = conv_block(ch_in=64,ch_out=128)\n",
    "        self.Conv3 = conv_block(ch_in=128,ch_out=256)\n",
    "        self.Conv4 = conv_block(ch_in=256,ch_out=512)\n",
    "        self.Conv5 = conv_block(ch_in=512,ch_out=1024)\n",
    "\n",
    "        self.Up5 = up_conv(ch_in=1024,ch_out=512)\n",
    "        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)\n",
    "\n",
    "        self.Up4 = up_conv(ch_in=512,ch_out=256)\n",
    "        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)\n",
    "        \n",
    "        self.Up3 = up_conv(ch_in=256,ch_out=128)\n",
    "        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)\n",
    "        \n",
    "        self.Up2 = up_conv(ch_in=128,ch_out=64)\n",
    "        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)\n",
    "\n",
    "        self.Conv_1x1 = single_conv(ch_in = 64, ch_out = output_ch)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # encoding path\n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "        \n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        d5 = torch.cat((x4,d5),dim=1)        \n",
    "        d5 = self.Up_conv5(d5)\n",
    "        \n",
    "        d4 = self.Up4(d5)\n",
    "        d4 = torch.cat((x3,d4),dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        d3 = torch.cat((x2,d3),dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = torch.cat((x1,d2),dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(outputs, labels):\n",
    "    outputs, labels = torch.sigmoid(outputs) > 0.5, labels > 0.5\n",
    "    SMOOTH = 1e-6\n",
    "    B, N, H, W = outputs.shape\n",
    "    ious = []\n",
    "    _out, _labs = outputs, labels\n",
    "    intersection = (_out & _labs).float().sum((1,2))\n",
    "    union = (_out | _labs).float().sum((1,2))\n",
    "    iou = (intersection + SMOOTH)/(union + SMOOTH)\n",
    "    ious.append(iou.mean().item())\n",
    "    return np.mean(ious)\n",
    "\n",
    "def DC(outputs, labels):\n",
    "    outputs, labels = torch.sigmoid(outputs)>0.5, labels > 0.5\n",
    "    SMOOTH = 1e-6\n",
    "    B, N, H, W = outputs.shape\n",
    "    DCs = []\n",
    "    _out, _labs = outputs, labels\n",
    "    intersection = (_out & _labs).float().sum((1,2))\n",
    "    DC = (2 * intersection)/(float(torch.sum(_out)+torch.sum(_labs))+SMOOTH)\n",
    "    DCs.append(DC.mean().item())\n",
    "    return np.mean(DCs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_vertical_flip(img, mask):\n",
    "    img = np.flipud(img)\n",
    "    mask = np.flipud(mask)\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4557e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_horizontal_flip(img, mask):\n",
    "    img = np.fliplr(img)\n",
    "    mask = np.fliplr(mask)\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotation(img, mask, bound_angle = 180):\n",
    "    angle = random.choice([*range(-bound_angle, bound_angle + 1)])\n",
    "    shape = img.shape[:2] # image may have 3 dimensions (height, width and channels). We only want the first two\n",
    "    center = np.array(shape) / 2\n",
    "    mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    img = cv2.warpAffine(img, mat, shape, flags=cv2.INTER_CUBIC)\n",
    "    mask = cv2.warpAffine(mask, mat, shape, flags=cv2.INTER_CUBIC)\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9306a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAugmentation:\n",
    "    augmentations = [apply_horizontal_flip, apply_vertical_flip, apply_rotation]\n",
    "    \n",
    "    def __init__(self, max_augment_count = 3):\n",
    "        if max_augment_count <= len(self.augmentations):\n",
    "            self.max_augment_count = max_augment_count\n",
    "        else:\n",
    "            self.max_augment_count = len(self.augmentations)\n",
    "            \n",
    "    def __call__(self, img, mask):\n",
    "        n_augm = random.randint(0, self.max_augment_count)\n",
    "        augms = random.sample(self.augmentations, k=n_augm)\n",
    "        for augm in augms:\n",
    "            img, mask = augm(img, mask)\n",
    "            \n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScanDataset(Dataset):\n",
    "    def __init__(self, data_dir, augmentations = RandomAugmentation(3)):\n",
    "        self.imgs_path = sorted(glob.glob(f'{data_dir}/img*'))\n",
    "        self.masks_path = sorted(glob.glob(f'{data_dir}/roi*'))\n",
    "        self.augmentations = augmentations\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_path)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_fn = self.imgs_path[idx]\n",
    "        mask_fn = self.masks_path[idx]\n",
    "        \n",
    "        img = nib.load(img_fn).get_data()\n",
    "        mask = nib.load(mask_fn).get_data()\n",
    "        mask[mask==2] = 0\n",
    "        \n",
    "        if self.augmentations:\n",
    "            img, mask = self.augmentations(img, mask)\n",
    "        \n",
    "        return self.to_tensor(img), self.to_tensor(mask)\n",
    "    \n",
    "    def to_tensor(self, mat):\n",
    "        mat = mat / 255\n",
    "        if mat.ndim == 2:\n",
    "            mat = np.expand_dims(mat, 0)\n",
    "        elif mat.ndim == 3:\n",
    "            mat = mat.transpose(2,0,1)\n",
    "        return torch.from_numpy(mat.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScanDataModule():\n",
    "    def __init__(self, data_dir, batch_size = 4, shuffle = True):\n",
    "        self.train_dataset = ScanDataset(os.path.join(data_dir, 'train'))\n",
    "        self.valid_dataset = ScanDataset(os.path.join(data_dir, 'valid'))\n",
    "        self.test_dataset = ScanDataset(os.path.join(data_dir, 'test'))\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def train_loader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size = self.batch_size, shuffle = self.shuffle, drop_last = True)\n",
    "    \n",
    "    def valid_loader(self):\n",
    "        return DataLoader(self.valid_dataset, batch_size = self.batch_size, shuffle = self.shuffle)\n",
    "    \n",
    "    def test_loader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size = self.batch_size, shuffle = self.shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = ScanDataModule(data_dir, batch_size = 4, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0ce0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, masks = next(iter(dataloader.train_loader()))\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 10))\n",
    "ax[0].imshow(imgs[0].squeeze(), cmap='gray', alpha = 1) \n",
    "ax[1].imshow(masks[0].squeeze(), cmap='gray', alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd4a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm #allows us to output a smart progress bar\n",
    "\n",
    "def fit(model, dataloader, epochs=150, lr=3e-4):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss() #loss\n",
    "    model.to(device) #move model to device, which is the GPU\n",
    "    hist = {'loss': [], 'iou': [], 'DC': [], 'evaluation_loss': [], 'evaluation_iou': [], 'evaluation_DC': []}\n",
    "    for epoch in range(1, epochs+1):\n",
    "        bar = tqdm(dataloader.train_loader()) #creates a smart progress bar for train data\n",
    "        train_loss, train_iou, train_DC = [], [], [] #create empty lists that are to be filled\n",
    "        model.train()\n",
    "        for imgs, masks in bar: #training the model\n",
    "            imgs, masks = imgs.float().to(device), masks.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(imgs)\n",
    "            y_hat = (y_hat - torch.min(y_hat))/(torch.max(y_hat) - torch.min(y_hat))\n",
    "            loss = criterion(y_hat, masks)\n",
    "            loss.backward()    #GRADIENT DECENT, adam optimizer\n",
    "            optimizer.step()   #updating model with the new gradients\n",
    "            ious = iou(y_hat, masks)\n",
    "            DCs = DC(y_hat, masks)\n",
    "            train_loss.append(loss.item())\n",
    "            train_iou.append(ious)\n",
    "            train_DC.append(DCs)\n",
    "            bar.set_description(f\"loss {np.mean(train_loss):.5f} iou {np.mean(train_iou):.5f} DC {np.mean(train_DC):.5f}\")\n",
    "        hist['loss'].append(np.mean(train_loss))\n",
    "        hist['iou'].append(np.mean(train_iou))\n",
    "        hist['DC'].append(np.mean(train_DC))\n",
    "        bar = tqdm(dataloader.valid_loader()) #creates a smart progress bar for evaluation data\n",
    "        evaluation_loss, evaluation_iou, evaluation_DC = [], [], [] #create empty lists for evaluation loss and iou\n",
    "        model.eval()\n",
    "        with torch.no_grad(): #evaluate the model\n",
    "            for imgs, masks in bar:\n",
    "                imgs, masks = imgs.float().to(device), masks.float().to(device)\n",
    "                y_hat = model(imgs)\n",
    "                y_hat = (y_hat - torch.min(y_hat))/(torch.max(y_hat) - torch.min(y_hat))\n",
    "                loss = criterion(y_hat, masks)\n",
    "                ious = iou(y_hat, masks)\n",
    "                DCs = DC(y_hat, masks)\n",
    "                evaluation_loss.append(loss.item())\n",
    "                evaluation_iou.append(ious)\n",
    "                evaluation_DC.append(DCs)\n",
    "                bar.set_description(f\"evaluation_loss {np.mean(evaluation_loss):.5f} evaluation_iou {np.mean(evaluation_iou):.5f} evaluation_DC {np.mean(evaluation_DC):.5f}\")\n",
    "        hist['evaluation_loss'].append(np.mean(evaluation_loss))\n",
    "        hist['evaluation_iou'].append(np.mean(evaluation_iou))\n",
    "        hist['evaluation_DC'].append(np.mean(evaluation_DC))\n",
    "        print(f\"\\nEpoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} iou {np.mean(train_iou):.5f} DC {np.mean(train_DC):.5f} evaluation_loss {np.mean(evaluation_loss):.5f} evaluation_iou {np.mean(evaluation_iou):.5f} evaluation_DC {np.mean(evaluation_DC):.5f}\")\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-discount",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model = Unet()\n",
    "hist = fit(model, dataloader, epochs=150)\n",
    "finish = time.time()\n",
    "timer = finish - start\n",
    "print(f'Unet execution time is: {timer} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(hist)\n",
    "df.plot(grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj, slic = 16, 16\n",
    "\n",
    "img = nib.load(f'{data_dir}/test/img_subj{subj:>03}_sl_{slic:>03}.nii').get_data()\n",
    "mask = nib.load(f'{data_dir}/test/roi_subj{subj:>03}_sl_{slic:>03}.nii').get_data()\n",
    "mask[mask==2] = 0\n",
    "\n",
    "img_tensor = torch.tensor(img).unsqueeze(0).unsqueeze(0)\n",
    "mask_tensor = torch.tensor(np.float32(mask)).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(img_tensor.float().to(device))\n",
    "    pred_mask = torch.argmax(output, axis = 0)\n",
    "\n",
    "np.unique(pred_mask.squeeze().cpu().numpy(), return_counts=True)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,10))\n",
    "ax1.imshow(img, cmap='gray')\n",
    "ax2.imshow(mask, cmap='gray')\n",
    "ax3.imshow(pred_mask.squeeze().cpu().numpy(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18080aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
